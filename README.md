# jpasa: Japanese Predicate-Argument Structure Analysis

This project explores deep learning models for Japanese Predicate-Argument Structure (PAS) Analysis, a task also known as Semantic Role Labeling (SRL). The primary goal is to identify the arguments corresponding to `ga`, `wo`, and `ni` cases for a given predicate in a Japanese sentence.

A key feature of this project is its focus on handling both standard dependent arguments and zero-anaphora (cases where an argument is omitted but implied by context), which is a significant challenge in Japanese NLP.

## Models and Methodology

The project implements and compares two distinct neural network architectures built on top of pre-trained transformer models.

### 1. Sequence Labeling (`SequenceLabelingBert.221009.0.py`)

This approach treats PAS analysis as a sequence labeling task. Each token in a sentence is classified with a label indicating its role relative to the predicate (e.g., `ga`-argument, `wo`-argument, `ni`-argument, or no-argument).

-   **Architecture**:
    1.  Embeddings are generated by a pre-trained transformer model.
    2.  These are concatenated with embeddings for additional features (e.g., part-of-speech).
    3.  A multi-layer bi-directional GRU processes the sequence of embeddings.
    4.  A final linear layer outputs a classification score for each token for each possible role.

### 2. Sequence Pointing (`SequencePointingBert.221022.0.py`)

This approach uses a pointing network. Instead of classifying every token, the model "points" to the single token that best represents the head of each argument type (`ga`, `wo`, `ni`).

-   **Architecture**:
    1.  The initial embedding and GRU layers are identical to the sequence labeling model.
    2.  The output layer consists of separate linear heads, one for each argument type. Each head produces a score across the entire sequence, and the position with the highest score is chosen as the location of that argument.

## Technology Stack

-   **Core Framework**: [PyTorch](https://pytorch.org/)
-   **Pre-trained Models**: The project is designed to be flexible and can utilize a variety of Japanese transformer models from the [Hugging Face Transformers](https://huggingface.co/models) library, including:
    -   `cl-tohoku/bert-base-japanese-whole-word-masking`
    -   `rinna/japanese-roberta-base`
    -   `nlp-waseda/roberta-base-japanese`
    -   `megagonlabs/t5-base-japanese-web`
    -   And several others.

## Code Structure

-   `SequenceLabelingBert.221009.0.py`: Main executable for training and evaluating the sequence labeling model.
-   `SequencePointingBert.221022.0.py`: Main executable for training and evaluating the sequence pointing model.
-   `Datasets.py`: Script for loading and preprocessing data from the NTC and BCCWJ corpora.
-   `Indexer.py`: A utility class to convert categorical features (like POS tags) into numerical indices.
-   `Validation.py`: Contains functions for calculating evaluation metrics, primarily F1-score, with special handling for "dependent" vs. "zero" arguments.
-   `BertJapaneseTokenizerFast.py`: A custom wrapper around the standard `BertJapaneseTokenizer` to provide character-level offset mappings, which are crucial for aligning model predictions with the original text.

## How to Run

1.  **Prepare Data**: Ensure the datasets (e.g., NTC, BCCWJ) are pre-processed into the `.pkl` format expected by `Datasets.py` and placed in the appropriate `../data/` directory.
2.  **Install Dependencies**: Install the required Python packages, including `torch`, `transformers`, `numpy`, and `tokenizations`.
3.  **Train a Model**: Execute one of the main scripts. The script will automatically iterate through a predefined list of transformer models, training and evaluating each one.

    ```bash
    # To run the sequence labeling models
    python SequenceLabelingBert.221009.0.py

    # To run the sequence pointing models
    python SequencePointingBert.221022.0.py
    ```

    The scripts will save training logs (`traininglog.json`), model configurations (`config.json`), and the best-performing model weights (`models.<epoch>.pth`) into a `results/` directory (the exact path is configured within the script).
